{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 끝난 HMM 의 parameters 인 emission prob. 와 transition prob. 의 log 값이 주어졌다고 가정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "emission = {\n",
    "    'Adjective': {'이': 0.1, '짧': 0.1},\n",
    "    'Eomi': {'다': 0.1, '았다': 0.1, '었다': 0.1},\n",
    "    'Josa': {'는': 0.15, '다': 0.05, '도': 0.1, '은': 0.2, '이': 0.1},\n",
    "    'Noun': {'시작': 0.1, '예시': 0.1, '이': 0.15, '이것': 0.1},\n",
    "    'Verb': {'입': 0.1, '하': 0.1}\n",
    "}\n",
    "\n",
    "transition = {\n",
    "    ('Adjective', 'Noun'): 0.1,\n",
    "    ('Josa', 'Adjective'): 0.1,\n",
    "    ('Josa', 'Noun'): 0.2,\n",
    "    ('Josa', 'Verb'): 0.1,\n",
    "    ('Noun', 'Adjective'): 0.05,\n",
    "    ('Noun', 'Josa'): 0.1,\n",
    "    ('Noun', 'Noun'): 0.1,\n",
    "    ('Noun', 'Verb'): 0.05,\n",
    "    ('Verb', 'Noun'): 0.1\n",
    "}\n",
    "\n",
    "begin = {\n",
    "    'Noun': 0.2,\n",
    "    'Verb': 0.1,\n",
    "    'Adjective': 0.1\n",
    "}\n",
    "\n",
    "_max_word_len = max(len(w) for words in emission.values() for w in words)\n",
    "_min_emission = min(s for words in emission.values() for s in words.values()) - 0.05\n",
    "_min_transition = min(transition.values()) - 0.05\n",
    "\n",
    "print(_max_word_len) # 2\n",
    "print(_min_emission) # 0.0\n",
    "print(_min_transition) # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainedHMMTagger:\n",
    "    def __init__(self, transition, emission, begin,\n",
    "        begin_state='BOS', end_state='EOS', unk_state='Unk'):\n",
    "\n",
    "        self.transition = transition\n",
    "        self.emission = emission\n",
    "        self.begin = begin\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_lookup(sentence):\n",
    "    sent = []\n",
    "    for eojeol in sentence.split():\n",
    "        sent += eojeol_lookup(eojeol, len(sent))\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eojeol_lookup(eojeol, offset=0):\n",
    "    n = len(eojeol)\n",
    "    pos = [[] for _ in range(n)]\n",
    "    for b in range(n):\n",
    "        for r in range(1, _max_word_len+1):\n",
    "            e = b + r\n",
    "            if e > n:\n",
    "                continue\n",
    "            surface = eojeol[b:e]\n",
    "            for tag in get_pos(surface):\n",
    "                pos[b].append((surface, tag, tag, b+offset, e+offset))\n",
    "    return pos\n",
    "\n",
    "def get_pos(sub):\n",
    "    tags = []\n",
    "    for tag, words in emission.items():\n",
    "        if sub in words:\n",
    "            tags.append(tag)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('예시', 'Noun', 'Noun', 0, 2)],\n",
       " [],\n",
       " [('이', 'Adjective', 'Adjective', 2, 3),\n",
       "  ('이', 'Josa', 'Josa', 2, 3),\n",
       "  ('이', 'Noun', 'Noun', 2, 3)],\n",
       " [('다', 'Eomi', 'Eomi', 3, 4), ('다', 'Josa', 'Josa', 3, 4)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eojeol_lookup(\"예시이다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('이', 'Adjective', 'Adjective', 0, 1),\n",
       "  ('이', 'Josa', 'Josa', 0, 1),\n",
       "  ('이', 'Noun', 'Noun', 0, 1),\n",
       "  ('이것', 'Noun', 'Noun', 0, 2)],\n",
       " [],\n",
       " [('은', 'Josa', 'Josa', 2, 3)],\n",
       " [('예시', 'Noun', 'Noun', 3, 5)],\n",
       " [],\n",
       " [('이', 'Adjective', 'Adjective', 5, 6),\n",
       "  ('이', 'Josa', 'Josa', 5, 6),\n",
       "  ('이', 'Noun', 'Noun', 5, 6)],\n",
       " [('다', 'Eomi', 'Eomi', 6, 7), ('다', 'Josa', 'Josa', 6, 7)]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lookup(\"이것은 예시이다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어간과 어미 분리를 해야한다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.lemmatizer import lemma_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word, i):\n",
    "    l = word[:i]\n",
    "    r = word[i:]\n",
    "    lemmas = []\n",
    "    len_word = len(word)\n",
    "    for l_, r_ in lemma_candidate(l, r):\n",
    "        word_ = l_ + ' + ' + r_\n",
    "        if (l_ in emission['Verb']) and (r_ in emission['Eomi']):\n",
    "            lemmas.append((word_, 'Verb', 'Eomi'))\n",
    "        if (l_ in emission['Adjective']) and (r_ in emission['Eomi']):\n",
    "            lemmas.append((word_, 'Adjective', 'Eomi'))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('이', 'Adjective', 'Adjective', 0, 1),\n",
       "  ('이', 'Josa', 'Josa', 0, 1),\n",
       "  ('이', 'Noun', 'Noun', 0, 1),\n",
       "  ('이것', 'Noun', 'Noun', 0, 2)],\n",
       " [],\n",
       " [('은', 'Josa', 'Josa', 2, 3)],\n",
       " [('예시', 'Noun', 'Noun', 3, 5)],\n",
       " [],\n",
       " [],\n",
       " [('다', 'Eomi', 'Eomi', 6, 7), ('다', 'Josa', 'Josa', 6, 7)]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lookup(\"이것은 예시였다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('이 + 었다', 'Adjective', 'Eomi')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize('였다',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize('있다',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eojeol_lookup(eojeol, offset=0):\n",
    "    n = len(eojeol)\n",
    "    pos = [[] for _ in range(n)]\n",
    "    for b in range(n):\n",
    "        for r in range(1, _max_word_len+1):\n",
    "            e = b + r\n",
    "            if e > n:\n",
    "                continue\n",
    "            surface = eojeol[b:e]\n",
    "            for tag in get_pos(surface):\n",
    "                pos[b].append((surface, tag, tag, b+offset, e+offset))\n",
    "            # 어절 단위로 쪼개는 과정\n",
    "            for i in range(1, e-b):\n",
    "                try:\n",
    "                    lemmas = lemmatize(surface, i)\n",
    "                    if lemmas:\n",
    "                        pos[b].extend([lemma + (b+offset, e+offset) for lemma in lemmas])\n",
    "                except TypeError: # lemmatize 할 수 없는 경우는 넘어감\n",
    "#                     print(surface, i)\n",
    "                    continue\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('이', 'Adjective', 'Adjective', 0, 1),\n",
       "  ('이', 'Josa', 'Josa', 0, 1),\n",
       "  ('이', 'Noun', 'Noun', 0, 1),\n",
       "  ('이것', 'Noun', 'Noun', 0, 2)],\n",
       " [],\n",
       " [('은', 'Josa', 'Josa', 2, 3)],\n",
       " [('예시', 'Noun', 'Noun', 3, 5)],\n",
       " [],\n",
       " [('이 + 었다', 'Adjective', 'Eomi', 5, 7)],\n",
       " [('다', 'Eomi', 'Eomi', 6, 7), ('다', 'Josa', 'Josa', 6, 7)]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lookup(\"이것은 예시였다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate (word, tag) graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞의 단어의 end index와 뒤 단어의 begin index가 같은 경우 이 둘을 연결한다.\n",
    "\n",
    "```python\n",
    "links = []\n",
    "for word in sent[:-1]:\n",
    "    for word in words:\n",
    "        begin = word[3]\n",
    "        end = word[4]\n",
    "        for adjacent in sent[end]:\n",
    "            links.append((word, adjacent))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonempty_first(sent, end, offset=0):\n",
    "    \"\"\"offset 이후의 지점에서 sent[i]가 empty가 아닌 가장 빠른 지점 리턴\"\"\"\n",
    "    for i in range(offset, end):\n",
    "        if sent[i]:\n",
    "            return i\n",
    "    return offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 끝 의미하는 EOS 추가\n",
    "\n",
    "```python\n",
    "sent = sentence_lookup(sentence)\n",
    "n_char = len(sent) + 1\n",
    "eos = ('EOS', 'EOS', 'EOS', n_char-1, n_char)\n",
    "sent.append([eos])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_link(sentence):\n",
    "    \n",
    "    chars = sentence.replace(' ', '')\n",
    "    sent = sentence_lookup(sentence)\n",
    "    n_char = len(sent) + 1\n",
    "    \n",
    "    eos = ('EOS', 'EOS', 'EOS', n_char-1, n_char)\n",
    "    sent.append([eos])\n",
    "    \n",
    "    # 첫 단어 위치\n",
    "    i = get_nonempty_first(sent, n_char)\n",
    "    \n",
    "    if i > 0:\n",
    "        sent[0].append((chars[:i], 'Unk', 'Unk', 0, i))\n",
    "        \n",
    "    links = []\n",
    "    for words in sent[:-1]:\n",
    "        for word in words:\n",
    "            begin = word[3]\n",
    "            end = word[4]\n",
    "            # 이을 단어가 없는 경우 'Unk' 추가\n",
    "            if not sent[end]:\n",
    "                b = get_nonempty_first(sent, n_char, end)\n",
    "                unk = (chars[end:b], 'Unk', 'Unk', end, b)\n",
    "                links.append((word, unk))\n",
    "            else:\n",
    "                # 아닌 경우 현재 단어의 끝점에서 시작하는 단어들과 이음\n",
    "                for adjacent in sent[end]:\n",
    "                    links.append((word, adjacent))\n",
    "    \n",
    "    # 'Unk'에서 시작하는 edge 생성\n",
    "    unks = {to_node for _, to_node in links if to_node[1] == 'Unk'}\n",
    "    for unk in unks:\n",
    "        for adjacent in sent[unk[3]]:\n",
    "            links.append((unk, adjacent))\n",
    "            \n",
    "    bos = ('BOS', 'BOS', 'BOS', 0, 0)\n",
    "    for word in sent[0]:\n",
    "        links.append((bos, word))\n",
    "    # 재정렬\n",
    "    links = sorted(links, key=lambda x:(x[0][3], x[1][4]))\n",
    "    \n",
    "    return links, bos, eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('BOS', 'BOS', 'BOS', 0, 0), ('이', 'Adjective', 'Adjective', 0, 1)),\n",
      " (('BOS', 'BOS', 'BOS', 0, 0), ('이', 'Josa', 'Josa', 0, 1)),\n",
      " (('BOS', 'BOS', 'BOS', 0, 0), ('이', 'Noun', 'Noun', 0, 1)),\n",
      " (('이', 'Adjective', 'Adjective', 0, 1), ('것', 'Unk', 'Unk', 1, 2)),\n",
      " (('이', 'Josa', 'Josa', 0, 1), ('것', 'Unk', 'Unk', 1, 2)),\n",
      " (('이', 'Noun', 'Noun', 0, 1), ('것', 'Unk', 'Unk', 1, 2)),\n",
      " (('BOS', 'BOS', 'BOS', 0, 0), ('이것', 'Noun', 'Noun', 0, 2)),\n",
      " (('이것', 'Noun', 'Noun', 0, 2), ('은', 'Josa', 'Josa', 2, 3)),\n",
      " (('은', 'Josa', 'Josa', 2, 3), ('예시', 'Noun', 'Noun', 3, 5)),\n",
      " (('예시', 'Noun', 'Noun', 3, 5), ('이 + 었다', 'Adjective', 'Eomi', 5, 7)),\n",
      " (('이 + 었다', 'Adjective', 'Eomi', 5, 7), ('EOS', 'EOS', 'EOS', 7, 8)),\n",
      " (('다', 'Eomi', 'Eomi', 6, 7), ('EOS', 'EOS', 'EOS', 7, 8)),\n",
      " (('다', 'Josa', 'Josa', 6, 7), ('EOS', 'EOS', 'EOS', 7, 8))]\n"
     ]
    }
   ],
   "source": [
    "links, bos, eos = generate_link('이것은 예시였다')\n",
    "\n",
    "pprint(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치는 앞 마디로부터 지금 단어로 이동하는 transition probability와 현재 마디의 단어, 품사가 발생할 emission probability의 곱(혹은 로그의 합)을 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weight(links):\n",
    "    \n",
    "    def weight(from_node, to_node):\n",
    "        morphs = to_node[0].split(' + ')\n",
    "        \n",
    "        # 첫 단어 점수\n",
    "        w = emission.get(to_node[1], {}).get(morphs[0], _min_emission)\n",
    "        w += transition.get((from_node[2], to_node[1]), _min_transition)\n",
    "        \n",
    "        # 두번째 단어 점수\n",
    "        if len(morphs) == 2:\n",
    "            w += emission.get(to_node[2], {}).get(morphs[1], _min_emission)\n",
    "            w += transition.get((from_node[2], to_node[2]), _min_transition)\n",
    "            \n",
    "        return w\n",
    "    \n",
    "    graph = []\n",
    "    for from_node, to_node in links:\n",
    "        edge = (from_node, to_node, weight(from_node, to_node))\n",
    "        graph.append(edge)\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('BOS', 'BOS', 'BOS', 0, 0), ('이', 'Adjective', 'Adjective', 0, 1), 0.1),\n",
      " (('BOS', 'BOS', 'BOS', 0, 0), ('이', 'Josa', 'Josa', 0, 1), 0.1),\n",
      " (('BOS', 'BOS', 'BOS', 0, 0), ('이', 'Noun', 'Noun', 0, 1), 0.15),\n",
      " (('이', 'Adjective', 'Adjective', 0, 1), ('것', 'Unk', 'Unk', 1, 2), 0.0),\n",
      " (('이', 'Josa', 'Josa', 0, 1), ('것', 'Unk', 'Unk', 1, 2), 0.0),\n",
      " (('이', 'Noun', 'Noun', 0, 1), ('것', 'Unk', 'Unk', 1, 2), 0.0),\n",
      " (('BOS', 'BOS', 'BOS', 0, 0), ('이것', 'Noun', 'Noun', 0, 2), 0.1),\n",
      " (('이것', 'Noun', 'Noun', 0, 2),\n",
      "  ('은', 'Josa', 'Josa', 2, 3),\n",
      "  0.30000000000000004),\n",
      " (('은', 'Josa', 'Josa', 2, 3),\n",
      "  ('예시', 'Noun', 'Noun', 3, 5),\n",
      "  0.30000000000000004),\n",
      " (('예시', 'Noun', 'Noun', 3, 5), ('이 + 었다', 'Adjective', 'Eomi', 5, 7), 0.25),\n",
      " (('이 + 었다', 'Adjective', 'Eomi', 5, 7), ('EOS', 'EOS', 'EOS', 7, 8), 0.0),\n",
      " (('다', 'Eomi', 'Eomi', 6, 7), ('EOS', 'EOS', 'EOS', 7, 8), 0.0),\n",
      " (('다', 'Josa', 'Josa', 6, 7), ('EOS', 'EOS', 'EOS', 7, 8), 0.0)]\n"
     ]
    }
   ],
   "source": [
    "graph = add_weight(links)\n",
    "pprint(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 포드 알고리즘을 이용한 최단경로 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ford_list(E, V, S, T):\n",
    "    \n",
    "    # 초기화\n",
    "    # (최대 가중치 + 1) * 노드 갯수\n",
    "    inf = (min((weight for from_, to_, weight in E)) - 1) * len(V)\n",
    "    \n",
    "    # 거리들\n",
    "    d = {node:0 if node == S else inf for node in V}\n",
    "    # 이전 노드\n",
    "    prev = {node:None for node in V}\n",
    "    \n",
    "    # 이터레이션\n",
    "    # 무한 루프 방지\n",
    "    for _ in range(len(V)):\n",
    "        # 이른 정지\n",
    "        changed = False\n",
    "        for u, v, Wuv in E:\n",
    "            d_new = d[u] + Wuv\n",
    "            if d_new > d[v]:\n",
    "                d[v] = d_new\n",
    "                prev[v] = u\n",
    "                changed = True\n",
    "        if not changed:\n",
    "            break\n",
    "            \n",
    "    # 순환 있는지 체크\n",
    "    for u, v, Wuv in E:\n",
    "        if d[u] + Wuv > d[v]:\n",
    "            raise ValueError('Cycle exists')\n",
    "            \n",
    "    # 패스 탐색\n",
    "    prev_ = prev[T]\n",
    "    if prev_ == S:\n",
    "        return path[::-1], d[T]\n",
    "    \n",
    "    path = [T]\n",
    "    while prev_ != S:\n",
    "        path.append(prev_)\n",
    "        prev_ = prev[prev_]\n",
    "    path.append(S)\n",
    "    \n",
    "    return path[::-1], d[T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('BOS', 'BOS', 'BOS', 0, 0), ('이', 'Adjective', 'Adjective', 0, 1), 0.1),\n",
       " (('BOS', 'BOS', 'BOS', 0, 0), ('이', 'Josa', 'Josa', 0, 1), 0.1),\n",
       " (('BOS', 'BOS', 'BOS', 0, 0), ('이', 'Noun', 'Noun', 0, 1), 0.15),\n",
       " (('이', 'Adjective', 'Adjective', 0, 1), ('것', 'Unk', 'Unk', 1, 2), 0.0),\n",
       " (('이', 'Josa', 'Josa', 0, 1), ('것', 'Unk', 'Unk', 1, 2), 0.0),\n",
       " (('이', 'Noun', 'Noun', 0, 1), ('것', 'Unk', 'Unk', 1, 2), 0.0),\n",
       " (('BOS', 'BOS', 'BOS', 0, 0), ('이것', 'Noun', 'Noun', 0, 2), 0.1),\n",
       " (('이것', 'Noun', 'Noun', 0, 2),\n",
       "  ('은', 'Josa', 'Josa', 2, 3),\n",
       "  0.30000000000000004),\n",
       " (('은', 'Josa', 'Josa', 2, 3),\n",
       "  ('예시', 'Noun', 'Noun', 3, 5),\n",
       "  0.30000000000000004),\n",
       " (('예시', 'Noun', 'Noun', 3, 5), ('이 + 었다', 'Adjective', 'Eomi', 5, 7), 0.25),\n",
       " (('이 + 었다', 'Adjective', 'Eomi', 5, 7), ('EOS', 'EOS', 'EOS', 7, 8), 0.0),\n",
       " (('다', 'Eomi', 'Eomi', 6, 7), ('EOS', 'EOS', 'EOS', 7, 8), 0.0),\n",
       " (('다', 'Josa', 'Josa', 6, 7), ('EOS', 'EOS', 'EOS', 7, 8), 0.0)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('BOS', 'BOS', 'BOS', 0, 0),\n",
       " ('EOS', 'EOS', 'EOS', 7, 8),\n",
       " ('것', 'Unk', 'Unk', 1, 2),\n",
       " ('다', 'Eomi', 'Eomi', 6, 7),\n",
       " ('다', 'Josa', 'Josa', 6, 7),\n",
       " ('예시', 'Noun', 'Noun', 3, 5),\n",
       " ('은', 'Josa', 'Josa', 2, 3),\n",
       " ('이', 'Adjective', 'Adjective', 0, 1),\n",
       " ('이', 'Josa', 'Josa', 0, 1),\n",
       " ('이', 'Noun', 'Noun', 0, 1),\n",
       " ('이 + 었다', 'Adjective', 'Eomi', 5, 7),\n",
       " ('이것', 'Noun', 'Noun', 0, 2)}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = {node for edge in graph for node in edge[:2]}\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적의 거리\n",
    "path, cost = ford_list(graph, nodes, bos, eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOS', 'BOS', 'BOS', 0, 0),\n",
       " ('이것', 'Noun', 'Noun', 0, 2),\n",
       " ('은', 'Josa', 'Josa', 2, 3),\n",
       " ('예시', 'Noun', 'Noun', 3, 5),\n",
       " ('이 + 었다', 'Adjective', 'Eomi', 5, 7),\n",
       " ('EOS', 'EOS', 'EOS', 7, 8)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9500000000000001"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석 결과로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(path):\n",
    "    pos = []\n",
    "    for word, tag0, tag1, b, e in path:\n",
    "        morphs = word.split(' + ')\n",
    "        pos.append((morphs[0], tag0))\n",
    "        if len(morphs) == 2:\n",
    "            pos.append((morphs[1], tag1))\n",
    "    return pos\n",
    "pos = flatten(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOS', 'BOS'),\n",
       " ('이것', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('예시', 'Noun'),\n",
       " ('이', 'Adjective'),\n",
       " ('었다', 'Eomi'),\n",
       " ('EOS', 'EOS')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미등록 단어의 경우는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOS', 'BOS'),\n",
       " ('tt', 'Unk'),\n",
       " ('도', 'Josa'),\n",
       " ('예시', 'Noun'),\n",
       " ('이', 'Adjective'),\n",
       " ('었다', 'Eomi'),\n",
       " ('EOS', 'EOS')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links, bos, eos = generate_link('tt도예시였다')\n",
    "graph = add_weight(links)\n",
    "nodes = {node for edge in graph for node in edge[:2]}\n",
    "path, cost = ford_list(graph, nodes, bos, eos)\n",
    "pos = flatten(path)\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unk 추정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOS', 'BOS'),\n",
       " ('tt', 'Unk'),\n",
       " ('도', 'Josa'),\n",
       " ('예시', 'Noun'),\n",
       " ('이', 'Adjective'),\n",
       " ('었다', 'Eomi')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Noun': 0.2, 'Verb': 0.1, 'Adjective': 0.1}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Noun': 0.2, 'Verb': 0.1, 'Adjective': 0.1}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{tag:prob for tag, prob in begin.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_unknown(pos):\n",
    "    pos_ = []\n",
    "    for i, pos_i in enumerate(pos[:-1]):\n",
    "        if not (pos_i[1] == 'Unk'):\n",
    "            pos_.append(pos_i)\n",
    "            continue\n",
    "        \n",
    "        # previous -> current transition 이용한 추정\n",
    "        if i == 1:\n",
    "            tag_prob = begin.copy()\n",
    "        else:\n",
    "            tag_prob = {\n",
    "                tag:prob for (prev_tag, tag), prob in transition.items()\n",
    "                if prev_tag == pos[i-1][1]\n",
    "            }\n",
    "            \n",
    "        # current -> next transition 이용한 추정\n",
    "        for (tag, next_tag), prob in transition.items():\n",
    "            if next_tag == pos[i+1][1]:\n",
    "                tag_prob[tag] = tag_prob.get(tag, 0) + prob\n",
    "                \n",
    "        # 전후 어떤 추정도 사용할 수 없으면 가장 많은 명사로 추정\n",
    "        if not tag_prob:\n",
    "            infered_tag = 'Noun'\n",
    "        else:\n",
    "            # 가장 확률 높은 태그 추천\n",
    "            infered_tag = sorted(tag_prob, key=lambda x:-tag_prob[x])[0]\n",
    "        pos_.append((pos_i[0], infered_tag))\n",
    "        \n",
    "    return pos_ + pos[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOS', 'BOS'),\n",
       " ('tt', 'Noun'),\n",
       " ('도', 'Josa'),\n",
       " ('예시', 'Noun'),\n",
       " ('이', 'Adjective'),\n",
       " ('었다', 'Eomi'),\n",
       " ('EOS', 'EOS')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_unknown(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(pos):\n",
    "    return pos[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_corpus\n",
    "sj = read_corpus('data/corpus_type1_all.txt', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('프랑스', 'Noun'),\n",
       "  ('의', 'Josa'),\n",
       "  ('세계적', 'Noun'),\n",
       "  ('인', 'Adjective'),\n",
       "  ('의상', 'Noun'),\n",
       "  ('디자이너', 'Noun'),\n",
       "  ('엠마누엘', 'Noun'),\n",
       "  ('웅가로', 'Noun'),\n",
       "  ('가', 'Josa'),\n",
       "  ('실내', 'Noun'),\n",
       "  ('장식용', 'Noun'),\n",
       "  ('직물', 'Noun'),\n",
       "  ('디자이너', 'Noun'),\n",
       "  ('로', 'Josa'),\n",
       "  ('나서', 'Verb'),\n",
       "  ('었다', 'Eomi')],\n",
       " [('웅가로', 'Noun'),\n",
       "  ('는', 'Josa'),\n",
       "  ('침실', 'Noun'),\n",
       "  ('과', 'Josa'),\n",
       "  ('식당', 'Noun'),\n",
       "  ('욕실', 'Noun'),\n",
       "  ('에서', 'Josa'),\n",
       "  ('사용', 'Noun'),\n",
       "  ('하는', 'Verb'),\n",
       "  ('갖가지', 'Noun'),\n",
       "  ('직물제품', 'Noun'),\n",
       "  ('을', 'Josa'),\n",
       "  ('디자인', 'Noun'),\n",
       "  ('해', 'Verb'),\n",
       "  ('최근', 'Noun'),\n",
       "  ('파리', 'Noun'),\n",
       "  ('의', 'Josa'),\n",
       "  ('갤러리', 'Noun'),\n",
       "  ('라파예트백화점', 'Noun'),\n",
       "  ('에서', 'Josa'),\n",
       "  ('색', 'Noun'),\n",
       "  ('의', 'Josa'),\n",
       "  ('컬렉션', 'Noun'),\n",
       "  ('이라는', 'Adjective'),\n",
       "  ('이름', 'Noun'),\n",
       "  ('으로', 'Josa'),\n",
       "  ('전시회', 'Noun'),\n",
       "  ('를', 'Josa'),\n",
       "  ('열', 'Verb'),\n",
       "  ('었다', 'Eomi')],\n",
       " [('목욕가운', 'Noun'),\n",
       "  ('부터', 'Josa'),\n",
       "  ('탁자보', 'Noun'),\n",
       "  ('냅킨', 'Noun'),\n",
       "  ('앞치마', 'Noun'),\n",
       "  ('까지', 'Josa'),\n",
       "  ('그', 'Pronoun'),\n",
       "  ('가', 'Josa'),\n",
       "  ('디자인', 'Noun'),\n",
       "  ('한', 'Verb'),\n",
       "  ('작품들', 'Noun'),\n",
       "  ('에서', 'Josa'),\n",
       "  ('두드러지', 'Verb'),\n",
       "  ('는', 'Eomi'),\n",
       "  ('것', 'Noun'),\n",
       "  ('은', 'Josa'),\n",
       "  ('색', 'Noun'),\n",
       "  ('의', 'Josa'),\n",
       "  ('조화', 'Noun'),\n",
       "  ('다', 'Josa')],\n",
       " [('남미풍', 'Noun'),\n",
       "  ('의', 'Josa'),\n",
       "  ('강렬', 'Noun'),\n",
       "  ('한', 'Adjective'),\n",
       "  ('원색끼리', 'Noun'),\n",
       "  ('의', 'Josa'),\n",
       "  ('조화', 'Noun'),\n",
       "  ('수채화', 'Noun'),\n",
       "  ('같이', 'Adverb'),\n",
       "  ('안온', 'Noun'),\n",
       "  ('한', 'Adjective'),\n",
       "  ('배색', 'Noun'),\n",
       "  ('등', 'Noun'),\n",
       "  ('색', 'Noun'),\n",
       "  ('의', 'Josa'),\n",
       "  ('분위기', 'Noun'),\n",
       "  ('를', 'Josa'),\n",
       "  ('강조', 'Noun'),\n",
       "  ('하는', 'Verb'),\n",
       "  ('기하학적', 'Noun'),\n",
       "  ('무늬', 'Noun'),\n",
       "  ('꽃무늬', 'Noun'),\n",
       "  ('디자인', 'Noun'),\n",
       "  ('이', 'Josa'),\n",
       "  ('주류', 'Noun'),\n",
       "  ('를', 'Josa'),\n",
       "  ('이루', 'Verb'),\n",
       "  ('고', 'Eomi'),\n",
       "  ('있', 'Verb'),\n",
       "  ('다', 'Eomi')],\n",
       " [('엠마누엘', 'Noun'),\n",
       "  ('웅가로', 'Noun'),\n",
       "  ('는', 'Josa'),\n",
       "  ('실내', 'Noun'),\n",
       "  ('장식품', 'Noun'),\n",
       "  ('을', 'Josa'),\n",
       "  ('디자인', 'Noun'),\n",
       "  ('할', 'Verb'),\n",
       "  ('때', 'Noun'),\n",
       "  ('옷', 'Noun'),\n",
       "  ('을', 'Josa'),\n",
       "  ('만들', 'Verb'),\n",
       "  ('ㄹ', 'Eomi'),\n",
       "  ('때', 'Noun'),\n",
       "  ('와는', 'Josa'),\n",
       "  ('다르', 'Adjective'),\n",
       "  ('ㄴ', 'Eomi'),\n",
       "  ('해방감', 'Noun'),\n",
       "  ('을', 'Josa'),\n",
       "  ('느끼', 'Verb'),\n",
       "  ('ㄴ다고', 'Eomi'),\n",
       "  ('말', 'Noun'),\n",
       "  ('한다', 'Verb')],\n",
       " [('집', 'Noun'),\n",
       "  ('은', 'Josa'),\n",
       "  ('창작', 'Noun'),\n",
       "  ('의', 'Josa'),\n",
       "  ('원천', 'Noun'),\n",
       "  ('이라는', 'Adjective'),\n",
       "  ('그', 'Pronoun'),\n",
       "  ('는', 'Josa'),\n",
       "  ('옷', 'Noun'),\n",
       "  ('못지않', 'Adjective'),\n",
       "  ('게', 'Eomi'),\n",
       "  ('공간', 'Noun'),\n",
       "  ('이', 'Josa'),\n",
       "  ('주', 'Verb'),\n",
       "  ('는', 'Eomi'),\n",
       "  ('미학', 'Noun'),\n",
       "  ('을', 'Josa'),\n",
       "  ('중요시', 'Noun'),\n",
       "  ('해', 'Verb'),\n",
       "  ('오', 'Verb'),\n",
       "  ('았다', 'Eomi')]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "from utils import as_bigram_tag\n",
    "\n",
    "def train(corpus, save_path):\n",
    "    pos2words = defaultdict(lambda: defaultdict(int))\n",
    "    trans = defaultdict(int)\n",
    "    bos = defaultdict(int)\n",
    "\n",
    "    # sent = [(word, tag), (word, tag), ... ] format\n",
    "    for sent in corpus:\n",
    "\n",
    "        # generation prob\n",
    "        for word, pos in sent:\n",
    "            pos2words[pos][word] += 1\n",
    "\n",
    "        # transition prob\n",
    "        for bigram in as_bigram_tag(sent):\n",
    "            trans[bigram] += 1\n",
    "\n",
    "        # begin prob (BOS -> tag)\n",
    "        bos[sent[0][1]] += 1\n",
    "\n",
    "        # end prob (tag -> EOS)\n",
    "        trans[\"_\".join([sent[-1][1], 'EOS'])] += 1\n",
    "        \n",
    "    # save trarined data\n",
    "    trained = dict()\n",
    "    \n",
    "    trained['pos2words'] = {k:dict(v) for k, v in pos2words.items()}\n",
    "    trained['trans'] = dict(trans)\n",
    "    trained['bos'] = dict(bos)\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(trained, f)\n",
    "        \n",
    "    return trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = train(sj, 'data/trained_corpus_type1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos2words': {'Noun': {'프랑스': 1,\n",
       "   '세계적': 1,\n",
       "   '의상': 1,\n",
       "   '디자이너': 2,\n",
       "   '엠마누엘': 2,\n",
       "   '웅가로': 3,\n",
       "   '실내': 2,\n",
       "   '장식용': 1,\n",
       "   '직물': 1,\n",
       "   '침실': 1,\n",
       "   '식당': 1,\n",
       "   '욕실': 1,\n",
       "   '사용': 1,\n",
       "   '갖가지': 1,\n",
       "   '직물제품': 1,\n",
       "   '디자인': 4,\n",
       "   '최근': 1,\n",
       "   '파리': 1,\n",
       "   '갤러리': 1,\n",
       "   '라파예트백화점': 1,\n",
       "   '색': 3,\n",
       "   '컬렉션': 1,\n",
       "   '이름': 1,\n",
       "   '전시회': 1,\n",
       "   '목욕가운': 1,\n",
       "   '탁자보': 1,\n",
       "   '냅킨': 1,\n",
       "   '앞치마': 1,\n",
       "   '작품들': 1,\n",
       "   '것': 1,\n",
       "   '조화': 2,\n",
       "   '남미풍': 1,\n",
       "   '강렬': 1,\n",
       "   '원색끼리': 1,\n",
       "   '수채화': 1,\n",
       "   '안온': 1,\n",
       "   '배색': 1,\n",
       "   '등': 1,\n",
       "   '분위기': 1,\n",
       "   '강조': 1,\n",
       "   '기하학적': 1,\n",
       "   '무늬': 1,\n",
       "   '꽃무늬': 1,\n",
       "   '주류': 1,\n",
       "   '장식품': 1,\n",
       "   '때': 2,\n",
       "   '옷': 2,\n",
       "   '해방감': 1,\n",
       "   '말': 1,\n",
       "   '집': 1,\n",
       "   '창작': 1,\n",
       "   '원천': 1,\n",
       "   '공간': 1,\n",
       "   '미학': 1,\n",
       "   '중요시': 1},\n",
       "  'Josa': {'의': 8,\n",
       "   '가': 2,\n",
       "   '로': 1,\n",
       "   '는': 3,\n",
       "   '과': 1,\n",
       "   '에서': 3,\n",
       "   '을': 5,\n",
       "   '으로': 1,\n",
       "   '를': 3,\n",
       "   '부터': 1,\n",
       "   '까지': 1,\n",
       "   '은': 2,\n",
       "   '다': 1,\n",
       "   '이': 2,\n",
       "   '와는': 1},\n",
       "  'Adjective': {'인': 1, '이라는': 2, '한': 2, '다르': 1, '못지않': 1},\n",
       "  'Verb': {'나서': 1,\n",
       "   '하는': 2,\n",
       "   '해': 2,\n",
       "   '열': 1,\n",
       "   '한': 1,\n",
       "   '두드러지': 1,\n",
       "   '이루': 1,\n",
       "   '있': 1,\n",
       "   '할': 1,\n",
       "   '만들': 1,\n",
       "   '느끼': 1,\n",
       "   '한다': 1,\n",
       "   '주': 1,\n",
       "   '오': 1},\n",
       "  'Eomi': {'었다': 2,\n",
       "   '는': 2,\n",
       "   '고': 1,\n",
       "   '다': 1,\n",
       "   'ㄹ': 1,\n",
       "   'ㄴ': 1,\n",
       "   'ㄴ다고': 1,\n",
       "   '게': 1,\n",
       "   '았다': 1},\n",
       "  'Pronoun': {'그': 2},\n",
       "  'Adverb': {'같이': 1}},\n",
       " 'trans': {'Noun_Josa': 33,\n",
       "  'Josa_Noun': 25,\n",
       "  'Noun_Adjective': 6,\n",
       "  'Adjective_Noun': 4,\n",
       "  'Noun_Noun': 21,\n",
       "  'Josa_Verb': 7,\n",
       "  'Verb_Eomi': 9,\n",
       "  'Eomi_EOS': 4,\n",
       "  'Noun_Verb': 7,\n",
       "  'Verb_Noun': 5,\n",
       "  'Josa_Pronoun': 1,\n",
       "  'Pronoun_Josa': 2,\n",
       "  'Eomi_Noun': 6,\n",
       "  'Josa_EOS': 1,\n",
       "  'Noun_Adverb': 1,\n",
       "  'Adverb_Noun': 1,\n",
       "  'Eomi_Verb': 1,\n",
       "  'Josa_Adjective': 1,\n",
       "  'Adjective_Eomi': 2,\n",
       "  'Verb_EOS': 1,\n",
       "  'Adjective_Pronoun': 1,\n",
       "  'Verb_Verb': 1},\n",
       " 'bos': {'Noun': 6}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
